---
title: "MDCE Assignment C, Group 9"
author: 
        - Quintijn de Leng - 6829376
        - Alec Noppe - 6947794
        - Guglielmo de Santis - 6664652
        - Anna Teixeira Rodeia - 6263747
date: "20-4-2022"
output: pdf_document
fig_caption: true
hold_position: true
---
\newpage
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries, message=FALSE, warning=FALSE, echo = FALSE}
library(mice)
library(dplyr)
library(ggplot2)
library(naniar)
library(tidyr)
library(caret)
library(cowplot)
library(Hmisc)
library(Metrics)
library(miceadds)
library(knitr)
```

```{r, echo = FALSE}
dfcom <- readRDS("complete_data.rds")
dfinc <- readRDS("incomplete_data_g9.rds")
```

```{r include=FALSE}
prep_ini <- mice(dfinc, maxit = 0)
prep_ini_meth <- prep_ini$method
prep_ini_meth[c("smoke", "active", "height", "bmi")] <- ""
prep_ini_meth["weight"] <- "~ I(bmi * (height / 100)^2)"
imp_prep <- mice(dfinc,
                m = 1,
                maxit = 1,
                method = prep_ini_meth,
                print = FALSE)
dfinc <- complete(imp_prep)
```

# Introduction

In this report the attempt to predict active heart rate in an incomplete dataset, using multiple imputation. Our goal is to deduce whether the imputed dataset can produce similar regression results as the complete dataset, and that there is not a consistent flaw in the imputation. To do this, we compare the prediction with the complete dataset. 

In Assignment A we saw that after a recalculation of cases in variable *weight* that were not actually missing, 16.8% of the data was missing. The missing data pattern was connected and not monotone. 

Also referring back to Assignment A, we consider the Missing at Random-assumption to be plausible because of a non-significant Little's test and relatively close estimations of means and variances under listwise deletion.

# Modeling Decisions

## Imputation model

Predictive Mean Matching (pmm) is used for the numeric variables *active*, *height* and *weight*. For the binary variable *smoke*, logistic imputation regression is used.

*BMI* is a derived variable and thus, passive imputation was used to preserve the relationship between *height* and *weight* and to fill in the missing data of BMI. 

## Predictors

As there are relatively few variables in this dataset, all remaining variables are used to imputate any variable, with the exception of *BMI* on *weight* and *height* to prevent circularity. The loss of information in the imputations for the 49 cases where *BMI* is known, but not *height* and *weight* is not considered of any importance, since only *BMI* is used in the scientifically interesting model.

## Imputation order

The default order for imputations is used, as there is no clear reason to specify this otherwise. The missing data are not clearly close to monotone and thus do not benefit from this order and derived variable *BMI* already follows *height* and *weight* and thus the updated value is already taken into account in the following iteration.

## Number of iterations

5 iterations were chosen as the default to start with. We did not see a convergence of the model yet, so this was increased to 20; after which the model showed convergence (see Appendix A, figure 1). This may be due to some obvious correlations between variables: *height* and *weight* are obviously related, and women are generally also shorter and have a lower weight.

## Number of imputed datasets

For the amount of imputations we followed the guideline to set this amount equal to the percentage of missing values (16.8%), rounded up to 17. We tried some higher and lower imputations as well, but this did not make a big impact on the plots. Because of this, we choose to keep the number of imputations equal to the percentage of missing values. 

## Final imputation model

Taking all of the former into account, the final imputation model looks like this:

```{r}
ini <- mice(dfinc, maxit = 0)
meth <- ini$method
meth["bmi"] <- "~ I(weight / (height / 100)^2)"
pred <- ini$predictorMatrix
pred[c("weight", "height"), "bmi"] <- 0

imp1 <- mice(dfinc,
             m = 17,
             maxit = 20,
             method = meth,
             predictorMatrix = pred,
             seed = 1337,
             print = FALSE)

```

Diagnostic plots of the imputations are offered in the Appendix. All point to a convergence of the model and realistic imputation outcomes.

## Means

In this section, means of the imputed, complete and incomplete dataset are compared. Only imputed numeric variables shown. As you cannot simply average the column means for each imputation *m*, the following calculation is used to come to the means:

```{r}
impmeans <- complete(imp1, "all") %>% 
            lapply(function(x) select(x, where(is.numeric)) %>% colMeans()) %>%
            do.call(rbind, .) %>%
            colMeans()
#From: Lang (2022, 3 December)
```



```{r, echo = FALSE, results = 'asis'}
impmeans <- as.data.frame(impmeans)
compmeans <- colMeans(dfcom[, c(1, 5:9)]) %>% 
        as.data.frame()
incmeans <- colMeans(dfinc[, c(1, 5:9)], na.rm = TRUE) %>% 
        as.data.frame()
meantable <- cbind(impmeans, compmeans, incmeans)
colnames(meantable) <- (c("imputed", "complete", "listwise"))
kable(round(meantable[c(2, 4:6), ], 2), caption = "Table of means in imputed, complete and incomplete dataset")
```

The mean of active heart rate in the imputed dataset is equal to the mean of the complete data. For *height*, the imputed dataset is actually further off than the listwise deletion method, as is slightly the case with *weight* (but overerstimating it instead of underestimating). However, this is still a minor difference. The imputed mean of *bmi* is closer to the complete data than the listwise-method is, although differences are very minor.

## Variances

In this section, the variance of the imputed, complete and incomplete dataset are compared. Only imputed numeric variables shown.

Variances for the imputed datasets are calculated as follows:

```{r}
impdat <- complete(imp1, action="long", include = FALSE)
pool_var <- with(impdat, by(impdat, .imp, function(x) c(var(x$active), 
                                                        var(x$height), 
                                                        var(x$weight), 
                                                        var(x$bmi))))
pool_var <- Reduce("+", pool_var) / length(pool_var)
#Adapted from: Heymans & Eekhout (2019)
```


```{r, echo = FALSE}
vartab <- as.data.frame(pool_var)


vartab$complete <- c(var(dfcom$active),
                     var(dfcom$height),
                     var(dfcom$weight),
                     var(dfcom$bmi))

vartab$listwise <- c(var(dfinc$active, na.rm = TRUE),
                     var(dfinc$height, na.rm = TRUE),
                     var(dfinc$weight, na.rm = TRUE),
                     var(dfinc$bmi, na.rm = TRUE))

colnames(vartab) <- c("imputed", "complete", "listwise")
rownames(vartab) <- c("active", "height", "weight", "bmi")
kable(round(vartab, 2), caption = "Table of variances for imputed, complete and incomplete data")
```

Unsurprisingly due to the added uncertainty of the missingness of the data, the variances are higher in the imputed datasets. However, the amount of added variance is relatively low. In the case of listwise deletion, an underestimation of the variance can even be seen in the event of *weight*.

## Correlations

In this section, matrices of the difference in correlations between the imputed and complete respectively the incomplete and complete datasets are shown.

```{r, echo = FALSE}
compcormat <- cor(dfcom[c(1, 5:9)])
inccormat <- cor(dfinc[c(1, 5:9)], use = "complete")
impcormat <- micombine.cor(mi.res = imp1, variables = c(1, 5:9))
impcormat <- attr(impcormat, "r_matrix")

diff1 <- impcormat - compcormat
diff2 <- inccormat - compcormat
diff3 <- diff1 - diff2
```


```{r, echo = FALSE}
kable(round((diff1), 2), caption = "Differences in correlations between the imputed and complete dataset")
```
\newpage

```{r, echo = FALSE}
kable(round((diff2), 2), caption = "Differences in correlations between the incomplete and complete dataset")
```


Looking at the differences between correlations, we can see that the imputed dataset slightly underestimates the correlation between *weight* and *height*, perhaps because *bmi* was not used in their imputations. Overall however, the correlations are much more preserved in the imputed dataset over listwise deletion. 

## *Smoke* frequencies

In this section, the difference in frequency of the *smoke* variable are considered. Frequencies for the imputed dataset are calculated as follows:

```{r}
pool_count <- with(impdat, by(impdat, .imp, function(x) summary(x$smoke)))
pool_count <- Reduce("+", pool_count) / length(pool_count)
#Adapted from: Heymans & Eekhout (2019)
```


```{r, echo = FALSE}
counttab <- as.data.frame(pool_count)
counttab$complete <- summary(dfcom$smoke)
counttab$listwise <- summary(dfinc$smoke)[1:2]
counttab[nrow(counttab) + 1,] <- colSums(counttab)
colnames(counttab) <- c("imputed", "complete", "listwise")
rownames(counttab) <- c("no", "yes", "total")
#kable(round(counttab, 0), caption = "Frequency table for *smoke*")
```

```{r, echo = FALSE}
countperc <- counttab[c(1:2), ]
for (x in 1:3) {
        for (y in 1:2) {
                countperc[y, x] <- counttab[y, x] / counttab [3, x]
        } 
}
countperc <- countperc * 100
kable(round(countperc, 1), caption = "Relative frequencies for *smoke*")
```

In both listwise and MI the fraction of non-smokers is slightly overestimated. The differences are, however, very minor.

# Scientifically Interesting Model

After having imputed the missing data, we want to investigate the performance of a linear model trained on the imputed data, as well as the model trained on the complete data. The goal of this section is to deduce whether the imputed dataset can produce similar regression results as the complete dataset, and that there is not a consistent flaw in the imputation. The code is shown in Appendix B.

*Active heart rate* is dependent on many factors. For this model, *active heart rate* is dependent on *age*, *BMI*, *resting heart rate*, *gender*, *smoking*, *intensity* and an interaction between *BMI* and *age*. These are all the variables included in the dataset, with an additional interaction between BMI and age. This is suggested to increase the active heartrate exponentially, as opposed to a linear addition (Watkinson et al., 2010). The most influential variables are intensity, gender, and age. These variables have the highest slopes, as shown in the table below. The table also shows that we have lost quite some information to the missing data. Especially the rest stage, respondents with the gender female and respondents that smoke show a high fmi.

```{r, echo = FALSE}
fit <- with(imp1, lm(active ~ age + rest + bmi + sex + smoke + intensity + bmi * age))
est <- pool(fit)
a <- as.data.frame(summary(est))
a$fmi <- est$pooled$fmi
a[,2:7] <- round(a[,2:7], 2)
colnames(a) <- c("term", "estimate", "SE", "statistic", "df", "p", "fmi")
rownames(a) <- c("(Intercept)", "age", "rest", "bmi", "sex (female)", "smoke (yes)", "intenstiy (moderate)", "intenstiy (low)", "age:bmi")
kable(a[,2:7], caption = "Estimates")
```

 

To assess a model's performance, it is important to evaluate the accuracy of the model on seen values (training validation) as well as the accuracy of the model on new data (testing validatieon). The reason for this is that a model may perform well on data that has been seen before, but not on new data. The metrics used to determine the performance of the model are the sum of squares error *SSE*, relative standard error *RSE*, root mean square error *RMSE* and *R^2*. For the former three metrics, a lower score is ideal. Whereas for the R^2, a score closest to 1 is ideal.

 


$$
SSE = \sum_{i=1}^{n}{(y_i - y_i')^2}
$$

$$
RSE = \sqrt{\frac{1}{n-2} \sum_{i=1}^{n}{(y_i- \hat{y_i})^2}}
$$
$$
RMSE = \sqrt{\sum_{i=1}^{n}{\frac{(\hat{y_i}-y_i)^2}{n}}}
$$

$$
R^2 = 1 - \frac{\sum_{i=1}^{n}{(y_i-\hat{y_i})^2}}{\sum_{i=1}^{n}{(y_i-\bar{y_i})^2}}
$$

## Training Validation
As seen below, the model trained on the imputed dataset produces more accurate results than the model trained on the complete dataset. The best imputed dataset model has an R^2 of 0.55, compared to the complete dataset model's 0.53. An R^2 of > 0.5, indicates quite good model fit. It shows that roughly 50% of the variation of *activity* is explained through the independent variables. This implies that there are some omitted variables that explain some of the activity variance. Nonetheless, the imputed dataset model performs similarly to the complete dataset model on known data.

```{r, echo = FALSE, results='hide', message=FALSE, warning=FALSE}
splitX <- function(imps, index) {
    tmp <- lapply(imps, split, f = index)
    out <- list()
    for(i in 1 : length(tmp[[1]]))
        out[[i]] <- lapply(tmp, "[[", x = i)
    names(out) <- names(tmp[[1]])
    out
}


modelmaker <- function(data, index) {
    a <- as.data.frame(data$one[index])
    colnames(a) <- c("age", "smoke", "sex", "intensity", 
                     "active", "rest", "height", "weight", "bmi")
    b <- as.data.frame(data$two[index])
    colnames(b) <- c("age", "smoke", "sex", "intensity", 
                     "active", "rest", "height", "weight", "bmi")
    c <- as.data.frame(data$three[index])
    colnames(c) <- c("age", "smoke", "sex", "intensity", 
                     "active", "rest", "height", "weight", "bmi") 
    d <- as.data.frame(data$four[index])
    colnames(d) <- c("age", "smoke", "sex", "intensity", 
                     "active", "rest", "height", "weight", "bmi")
    e <- as.data.frame(data$five[index])
    colnames(e) <- c("age", "smoke", "sex", "intensity", 
                     "active", "rest", "height", "weight", "bmi")
    t1 <- rbind(a,b)
    t2 <- rbind(t1,c)
    df <- rbind(t2,d)
    out <- lm(active ~ age + rest + bmi + sex + smoke + intensity + bmi*age, data=df)
    val <- list(predict(out, e))
    return(val)
    
}

trainModelmaker <- function(data, index) {
    a <- as.data.frame(data$one[index])
    colnames(a) <- c("age", "smoke", "sex", "intensity", 
                     "active", "rest", "height", "weight", "bmi")
    b <- as.data.frame(data$two[index])
    colnames(b) <- c("age", "smoke", "sex", "intensity", 
                     "active", "rest", "height", "weight", "bmi")
    c <- as.data.frame(data$three[index])
    colnames(c) <- c("age", "smoke", "sex", "intensity", 
                     "active", "rest", "height", "weight", "bmi") 
    d <- as.data.frame(data$four[index])
    colnames(d) <- c("age", "smoke", "sex", "intensity", 
                     "active", "rest", "height", "weight", "bmi")
    e <- as.data.frame(data$five[index])
    colnames(e) <- c("age", "smoke", "sex", "intensity", 
                     "active", "rest", "height", "weight", "bmi")
    t1 <- rbind(a,b)
    t2 <- rbind(t1,c)
    df <- rbind(t2,d)
    out <- lm(active ~ age + rest + bmi + sex + smoke + intensity + bmi*age, data=df)
    val <- list(predict(out, df))
    return(val)
    
}

fetchTrainData <- function(data, index) {
        a <- as.data.frame(data$one[index])
    colnames(a) <- c("age", "smoke", "sex", "intensity", 
                     "active", "rest", "height", "weight", "bmi")
    b <- as.data.frame(data$two[index])
    colnames(b) <- c("age", "smoke", "sex", "intensity", 
                     "active", "rest", "height", "weight", "bmi")
    c <- as.data.frame(data$three[index])
    colnames(c) <- c("age", "smoke", "sex", "intensity", 
                     "active", "rest", "height", "weight", "bmi") 
    d <- as.data.frame(data$four[index])
    colnames(d) <- c("age", "smoke", "sex", "intensity", 
                     "active", "rest", "height", "weight", "bmi")
    t1 <- rbind(a,b)
    t2 <- rbind(t1,c)
    df <- rbind(t2,d)
    df
}

fetchData <- function(data, index) {
        a <- as.data.frame(data$five[m])
        colnames(a) <- c("age", "smoke", "sex", "intensity", 
                         "active", "rest", "height", "weight", "bmi")
        a
}

trainValidator <- function(data, pred, index, sigh) {
        a <- as.data.frame(data$one[index])
        colnames(a) <- c("age", "smoke", "sex", "intensity", 
                     "active", "rest", "height", "weight", "bmi")
        b <- as.data.frame(data$two[index])
        colnames(b) <- c("age", "smoke", "sex", "intensity", 
                     "active", "rest", "height", "weight", "bmi")
        c <- as.data.frame(data$three[index])
        colnames(c) <- c("age", "smoke", "sex", "intensity", 
                     "active", "rest", "height", "weight", "bmi") 
        d <- as.data.frame(data$four[index])
        colnames(d) <- c("age", "smoke", "sex", "intensity", 
                     "active", "rest", "height", "weight", "bmi")
        t1 <- rbind(a,b)
        t2 <- rbind(t1,c)
        e <- rbind(t2,d)
        e
        
        f <- as.data.frame(pred)
        colnames(f) <- c("active")
        tActivity <- e$active
        mActivity <- f$active 
        test.com.active<- tActivity
        pred.com.error <- tActivity - mActivity
        pred.com.R2=1-sum(pred.com.error^2)/sum((test.com.active- mean(test.com.active))^2)
        if (sigh=="r2")
                return(pred.com.R2)
        else if(sigh=="sse")
                return(sse(tActivity, mActivity))
        else if(sigh=="rse")
                return(rse(tActivity, mActivity))
        else if(sigh=="rmse")
                return(rmse(tActivity, mActivity))
}

validator <- function(data, pred, index, sigh) {
        e <- as.data.frame(data$five[index])
        colnames(e) <- c("age", "smoke", "sex", "intensity", 
                         "active", "rest", "height", "weight", "bmi")
        f <- as.data.frame(pred)
        colnames(f) <- c("active")
        tActivity <- e$active
        mActivity <- f$active 
        test.com.active<- tActivity
        pred.com.error <- tActivity - mActivity
        pred.com.R2=1-sum(pred.com.error^2)/sum((test.com.active- mean(test.com.active))^2)
        if (sigh=="r2")
                return(pred.com.R2)
        else if(sigh=="sse")
                return(sse(tActivity, mActivity))
        else if(sigh=="rse")
                return(rse(tActivity, mActivity))
        else if(sigh=="rmse")
                return(rmse(tActivity, mActivity))
}

fixer <- function(data) {
        df <- t(as.data.frame(data))
        colnames(df) <- c("Metric")
        mean(df)
}

findX <- function(data, type) {
        df <- t(as.data.frame(data))
        colnames(df) <- c("Metric")
        if(type=="max")
                return(max(df))
        else if(type=="min")
                return(min(df))
}
d <- splitX(complete(imp1, "all"), c("one","two","three","four","five"))
```

```{r, echo = FALSE}
fullcom <- complete(imp1, "long")
smp_size <- floor(0.75 * nrow(dfcom))
smp_comp_size <- floor(0.75 * nrow(fullcom))
 
## set the seed to make your partition reproducible
set.seed(123)
train_ind <- sample(seq_len(nrow(dfcom)), size = smp_size)
train_full_ind <- sample(seq_len(nrow(fullcom)), size = smp_comp_size)
 
train.com <- dfcom[train_ind, ]
test.com <- dfcom[-train_ind, ]

 

model.com <- lm(active ~ age + rest + bmi + sex + smoke + intensity + bmi * age, data=train.com)
pred.com <- predict(model.com, newdata=train.com)


tActivity <- train.com$active
mActivity <- pred.com

test.com.active<- tActivity
pred.com.error <- tActivity - mActivity
pred.com.R2=1-sum(pred.com.error^2)/sum((test.com.active- mean(test.com.active))^2)
eval.com <- c(sse(tActivity, mActivity), rse(tActivity, mActivity), rmse(tActivity, mActivity), pred.com.R2)
eval.p <- eval.com
```

```{r, echo = FALSE}
predictions <- list()

for (m in 1 : length(d$one))
        predictions[m] <- trainModelmaker(d, m)

predSSE <- list()
predRSE <- list()
predRMSE <- list()
predR2 <- list()
for (m in 1: length(predictions))
    predSSE[m] <- trainValidator(d, predictions[m], m, "sse")

for (m in 1: length(predictions))
    predRSE[m] <- trainValidator(d, predictions[m], m, "rse")

for (m in 1: length(predictions))
    predRMSE[m] <- trainValidator(d, predictions[m], m, "rmse")

for (m in 1: length(predictions))
    predR2[m] <- trainValidator(d, predictions[m], m, "r2")

eval.com <- list()

eval.com[1] <- fixer(predSSE)
eval.com[2] <- fixer(predRSE)
eval.com[3] <- fixer(predRMSE)
eval.com[4] <- fixer(predR2)

eval.best <- list()
eval.best[1] <- findX(predSSE, "min")
eval.best[2] <- findX(predRSE, "min")
eval.best[3] <- findX(predRMSE, "min")
eval.best[4] <- findX(predR2, "max")

eval.worst <- list()
eval.worst[1] <- findX(predSSE, "max")
eval.worst[2] <- findX(predRSE, "max")
eval.worst[3] <- findX(predRMSE, "max")
eval.worst[4] <- findX(predR2, "min")


eval1 <- as.data.frame(eval.com)
eval2 <- rbind(eval1,eval.best)
eval3 <- rbind(eval2,eval.worst)
eval <- rbind(eval3, eval.p)
rownames(eval) <- c("Average (Imputed)", "Best (Imputed)", "Worst (Imputed)", "Complete")
colnames(eval) <- c("SSE", "RSE", "RMSE", "R2")
kable(round(eval, 2), caption = "Training results")
```

## Testing Validation

The best imputed dataset model has a high testing accuracy than the complete dataset. The table below shows the chosen metrics for the imputed datasets, as well as the complete dataset. The best imputed dataset has an R^2 score of 0.49, whereas the complete dataset model has a R^2 of 0.46. The average testing accuracy of the imputed model is significantly lower than the complete dataset model. It has an average R^2 of 0.37. This shows there is great variance between the different imputed sets. We expect the accuracy of the imputed model to be slightly lower, but the results as shown through these metrics can be attributed to multiple things. The test validation set may be too small to produce consistent results. A train-test split of 0.8-0.2 was chosen for this model. There are a total of 306 rows, thus the test validation set is only 61 rows long. It is also possible that there is an omitted variable that systematically influences the other variables, that has a larger effect on the outcome. A possible omitted variable could be whether the patient has any respiratory issues, since this is proven to lead to increased active heart rate. 
But the results do suggest that the best imputed dataset model yields similar results to the complete dataset model. The only relevant model is the one that performs best, so if we had to predict more unknown heart rates, we would use the best imputed dataset (R^2 of 0.49). Thus, the multiple imputation as performed in this research successfully fills in the missing values to an extent that yields proper results for our scientifically interesting model.
The model shows significant signs of overfitting to the training data. The training R^2 of the best imputed dataset and complete dataset are 0.10 and 0.06 higher respectively. To improve the fitting of the model, a future implementation could use K-Fold cross validation. This allows for an easier approach to maximize the testing accuracy, since you have more subsets to compare.
```{r, echo = FALSE}

 

model.com <- lm(active ~ age + rest + bmi + sex + smoke + intensity + bmi * age, data=train.com)
pred.com <- predict(model.com, newdata=test.com)

tActivity <- test.com$active
mActivity <- pred.com

test.com.active<- tActivity
pred.com.error <- tActivity - mActivity
pred.com.R2=1-sum(pred.com.error^2)/sum((test.com.active- mean(test.com.active))^2)
eval.com <- c(sse(tActivity, mActivity), rse(tActivity, mActivity), rmse(tActivity, mActivity), pred.com.R2)
eval.p <- eval.com
 
```

```{r, echo=FALSE}

predictions <- list()

for (m in 1 : length(d$one))
        predictions[m] <- modelmaker(d, m)

predSSE <- list()
predRSE <- list()
predRMSE <- list()
predR2 <- list()
for (m in 1: length(predictions))
    predSSE[m] <- validator(d, predictions[m], m, "sse")

for (m in 1: length(predictions))
    predRSE[m] <- validator(d, predictions[m], m, "rse")

for (m in 1: length(predictions))
    predRMSE[m] <- validator(d, predictions[m], m, "rmse")

for (m in 1: length(predictions))
    predR2[m] <- validator(d, predictions[m], m, "r2")

eval.com <- list()

eval.com[1] <- fixer(predSSE)
eval.com[2] <- fixer(predRSE)
eval.com[3] <- fixer(predRMSE)
eval.com[4] <- fixer(predR2)

eval.best <- list()
eval.best[1] <- findX(predSSE, "min")
eval.best[2] <- findX(predRSE, "min")
eval.best[3] <- findX(predRMSE, "min")
eval.best[4] <- findX(predR2, "max")

eval.worst <- list()
eval.worst[1] <- findX(predSSE, "max")
eval.worst[2] <- findX(predRSE, "max")
eval.worst[3] <- findX(predRMSE, "max")
eval.worst[4] <- findX(predR2, "min")


eval1 <- as.data.frame(eval.com)
eval2 <- rbind(eval1,eval.best)
eval3 <- rbind(eval2,eval.worst)
eval <- rbind(eval3, eval.p)
rownames(eval) <- c("Average (Imputed)", "Best (Imputed)", "Worst (Imputed)", "Complete")
colnames(eval) <- c("SSE", "RSE", "RMSE", "R2")
kable(round(eval, 2), caption = "Testing results")
```


## Analysis of Variance

Finally, an Analysis of variance *ANOVA* analysis is performed on both datasets to conclude whether there is a difference in relative variable importance between the imputed dataset model, and the complete dataset model. ANOVA is a statistical technique that is used to check if there are dominant variables in the chosen model. Each line represents a variable, and the metrics show to what extent it influences the outcome of the regression. In the imputed dataset, the most significant variables in terms of F-scores are age and resting heart rate. These variables have the highest F-values and the lowest p-values. Other variables show noteworthy variance, but none have a p-score lower than the significance level (0.05).
In the complete dataset, there are four variables under the significane level. These variables are: age, rest, gender, and the interaction between age & bmi. Since the imputed dataset only finds two significant variables, we conclude that some information is systematically misinterpreted by the imputations. We know that some information is lost, because the complete dataset has a lower residual sum of squares (54764 < 56539).
However, the residual sum of squares is within 2% of eachother. This shows that the imputed dataset model can produce similar results to the complete dataset model, regardless of the difference in variable importance. An explanation for this could be there is an omitted variable that influences the observed variables in a systematic way that is captured partially by the imputed dataset, and partially by the complete dataset. This would explain why there are different variable importances, but a similar residual sum of squares. An example used before is the presence of respiratory issues, which are not dependent on age or resting heartrate, but do inflate the active heart rate. The imputations may inflate a different variable to adjust for this unexplained variance, which results in the different variable importance shown below.

```{r include=FALSE}
a <- miceadds::mi.anova(imp1, "active ~ age + rest + bmi + sex + smoke + intensity + bmi*age", type=2)
aov <- aov(active ~ rest + bmi + sex + intensity + bmi*age, dfcom)
b <- summary(aov(active ~ rest + bmi + sex + intensity + bmi*age, dfcom))
```

```{r, echo=FALSE}
kable(a$anova.table, caption = "ANOVA table for imputed data")
```

```{r, echo = FALSE}
kable(b[[1]], caption = "ANOVA table for complete data")
```


\newpage
## Sources

- Chunk 6 (imputation model means): Lang, K. (2022, 3 december). *Missing Data Theory & Causal Effects, practical 6, sec. 2.2* [R code]. 
- Chunks 8, 13 (imputation model variances; *smoke* frequencies): Heymans, M.W. & Eekhout, I. (2019). *Applied Missing Data Analysis with SPSS and Rstudio*, section 5.2.2 [R code].
- Watkinson, C., van Sluijs, E. M., Sutton, S., Hardeman, W., Corder, K., & Griffin, S. J. (2010). Overestimation of physical activity level is associated with lower BMI: a cross-sectional analysis. *International journal of behavioral nutrition and physical activity, 7*(1), 1-9.
- Chunk 23 (Splitting the imputed models): Lang, K. (2022, 15 april). *Missing Data Theory & Causal Effects, miPredictionRoutines.R* [R code].
\newpage

## Appendix A: diagnostic figures

```{r convergence, echo = FALSE, fig.height = 6.7, fig.cap = "Convergence of imputed data"}
plot(imp1,
     layout = c(2, 5))
```

```{r densityplot, echo = FALSE, out.width = "90%", fig.cap = "Density plot of imputed data"}
densityplot(imp1)
```

```{r stripplot, echo = FALSE, out.width = "90%", fig.cap = "Stripplot of imputed data"}
stripplot(imp1)
```

\newpage

## Appendix B: Prediction Routine

```{r predcom}

 

model.com <- lm(active ~ age + rest + bmi + sex + smoke + intensity + bmi * age, data=train.com)
pred.com <- predict(model.com, newdata=test.com)

tActivity <- test.com$active
mActivity <- pred.com

test.com.active<- tActivity
pred.com.error <- tActivity - mActivity
pred.com.R2=1-sum(pred.com.error^2)/sum((test.com.active- mean(test.com.active))^2)
eval.com <- c(sse(tActivity, mActivity), rse(tActivity, mActivity), rmse(tActivity, mActivity), pred.com.R2)
eval.p <- eval.com
 
```

```{r predimp, echo = T, results='hide', message=FALSE, warning=FALSE}
splitX <- function(imps, index) {
    tmp <- lapply(imps, split, f = index)
    out <- list()
    for(i in 1 : length(tmp[[1]]))
        out[[i]] <- lapply(tmp, "[[", x = i)
    names(out) <- names(tmp[[1]])
    out
}


modelmaker <- function(data, index) {
    a <- as.data.frame(data$one[index])
    colnames(a) <- c("age", "smoke", "sex", "intensity", 
                     "active", "rest", "height", "weight", "bmi")
    b <- as.data.frame(data$two[index])
    colnames(b) <- c("age", "smoke", "sex", "intensity", 
                     "active", "rest", "height", "weight", "bmi")
    c <- as.data.frame(data$three[index])
    colnames(c) <- c("age", "smoke", "sex", "intensity", 
                     "active", "rest", "height", "weight", "bmi") 
    d <- as.data.frame(data$four[index])
    colnames(d) <- c("age", "smoke", "sex", "intensity", 
                     "active", "rest", "height", "weight", "bmi")
    e <- as.data.frame(data$five[index])
    colnames(e) <- c("age", "smoke", "sex", "intensity", 
                     "active", "rest", "height", "weight", "bmi")
    t1 <- rbind(a,b)
    t2 <- rbind(t1,c)
    df <- rbind(t2,d)
    out <- lm(active ~ age + rest + bmi + sex + smoke + intensity + bmi*age, data=df)
    val <- list(predict(out, e))
    return(val)
    
}

trainModelmaker <- function(data, index) {
    a <- as.data.frame(data$one[index])
    colnames(a) <- c("age", "smoke", "sex", "intensity", 
                     "active", "rest", "height", "weight", "bmi")
    b <- as.data.frame(data$two[index])
    colnames(b) <- c("age", "smoke", "sex", "intensity", 
                     "active", "rest", "height", "weight", "bmi")
    c <- as.data.frame(data$three[index])
    colnames(c) <- c("age", "smoke", "sex", "intensity", 
                     "active", "rest", "height", "weight", "bmi") 
    d <- as.data.frame(data$four[index])
    colnames(d) <- c("age", "smoke", "sex", "intensity", 
                     "active", "rest", "height", "weight", "bmi")
    e <- as.data.frame(data$five[index])
    colnames(e) <- c("age", "smoke", "sex", "intensity", 
                     "active", "rest", "height", "weight", "bmi")
    t1 <- rbind(a,b)
    t2 <- rbind(t1,c)
    df <- rbind(t2,d)
    out <- lm(active ~ age + rest + bmi + sex + smoke + intensity + bmi*age, data=df)
    val <- list(predict(out, df))
    return(val)
    
}

fetchTrainData <- function(data, index) {
        a <- as.data.frame(data$one[index])
    colnames(a) <- c("age", "smoke", "sex", "intensity", 
                     "active", "rest", "height", "weight", "bmi")
    b <- as.data.frame(data$two[index])
    colnames(b) <- c("age", "smoke", "sex", "intensity", 
                     "active", "rest", "height", "weight", "bmi")
    c <- as.data.frame(data$three[index])
    colnames(c) <- c("age", "smoke", "sex", "intensity", 
                     "active", "rest", "height", "weight", "bmi") 
    d <- as.data.frame(data$four[index])
    colnames(d) <- c("age", "smoke", "sex", "intensity", 
                     "active", "rest", "height", "weight", "bmi")
    t1 <- rbind(a,b)
    t2 <- rbind(t1,c)
    df <- rbind(t2,d)
    df
}

fetchData <- function(data, index) {
        a <- as.data.frame(data$five[m])
        colnames(a) <- c("age", "smoke", "sex", "intensity", 
                         "active", "rest", "height", "weight", "bmi")
        a
}

trainValidator <- function(data, pred, index, sigh) {
        a <- as.data.frame(data$one[index])
        colnames(a) <- c("age", "smoke", "sex", "intensity", 
                     "active", "rest", "height", "weight", "bmi")
        b <- as.data.frame(data$two[index])
        colnames(b) <- c("age", "smoke", "sex", "intensity", 
                     "active", "rest", "height", "weight", "bmi")
        c <- as.data.frame(data$three[index])
        colnames(c) <- c("age", "smoke", "sex", "intensity", 
                     "active", "rest", "height", "weight", "bmi") 
        d <- as.data.frame(data$four[index])
        colnames(d) <- c("age", "smoke", "sex", "intensity", 
                     "active", "rest", "height", "weight", "bmi")
        t1 <- rbind(a,b)
        t2 <- rbind(t1,c)
        e <- rbind(t2,d)
        e
        
        f <- as.data.frame(pred)
        colnames(f) <- c("active")
        tActivity <- e$active
        mActivity <- f$active 
        test.com.active<- tActivity
        pred.com.error <- tActivity - mActivity
        pred.com.R2=1-sum(pred.com.error^2)/sum((test.com.active- mean(test.com.active))^2)
        if (sigh=="r2")
                return(pred.com.R2)
        else if(sigh=="sse")
                return(sse(tActivity, mActivity))
        else if(sigh=="rse")
                return(rse(tActivity, mActivity))
        else if(sigh=="rmse")
                return(rmse(tActivity, mActivity))
}

validator <- function(data, pred, index, sigh) {
        e <- as.data.frame(data$five[index])
        colnames(e) <- c("age", "smoke", "sex", "intensity", 
                         "active", "rest", "height", "weight", "bmi")
        f <- as.data.frame(pred)
        colnames(f) <- c("active")
        tActivity <- e$active
        mActivity <- f$active 
        test.com.active<- tActivity
        pred.com.error <- tActivity - mActivity
        pred.com.R2=1-sum(pred.com.error^2)/sum((test.com.active- mean(test.com.active))^2)
        if (sigh=="r2")
                return(pred.com.R2)
        else if(sigh=="sse")
                return(sse(tActivity, mActivity))
        else if(sigh=="rse")
                return(rse(tActivity, mActivity))
        else if(sigh=="rmse")
                return(rmse(tActivity, mActivity))
}

fixer <- function(data) {
        df <- t(as.data.frame(data))
        colnames(df) <- c("Metric")
        mean(df)
}

findX <- function(data, type) {
        df <- t(as.data.frame(data))
        colnames(df) <- c("Metric")
        if(type=="max")
                return(max(df))
        else if(type=="min")
                return(min(df))
}
d <- splitX(complete(imp1, "all"), c("one","two","three","four","five"))
```

```{r predres, echo=T}

predictions <- list()

for (m in 1 : length(d$one))
        predictions[m] <- modelmaker(d, m)

predSSE <- list()
predRSE <- list()
predRMSE <- list()
predR2 <- list()
for (m in 1: length(predictions))
    predSSE[m] <- validator(d, predictions[m], m, "sse")

for (m in 1: length(predictions))
    predRSE[m] <- validator(d, predictions[m], m, "rse")

for (m in 1: length(predictions))
    predRMSE[m] <- validator(d, predictions[m], m, "rmse")

for (m in 1: length(predictions))
    predR2[m] <- validator(d, predictions[m], m, "r2")

eval.com <- list()

eval.com[1] <- fixer(predSSE)
eval.com[2] <- fixer(predRSE)
eval.com[3] <- fixer(predRMSE)
eval.com[4] <- fixer(predR2)

eval.best <- list()
eval.best[1] <- findX(predSSE, "min")
eval.best[2] <- findX(predRSE, "min")
eval.best[3] <- findX(predRMSE, "min")
eval.best[4] <- findX(predR2, "max")

eval.worst <- list()
eval.worst[1] <- findX(predSSE, "max")
eval.worst[2] <- findX(predRSE, "max")
eval.worst[3] <- findX(predRMSE, "max")
eval.worst[4] <- findX(predR2, "min")


eval1 <- as.data.frame(eval.com)
eval2 <- rbind(eval1,eval.best)
eval3 <- rbind(eval2,eval.worst)
eval <- rbind(eval3, eval.p)
rownames(eval) <- c("Average (Imputed)", "Best (Imputed)", "Worst (Imputed)", "Complete")
colnames(eval) <- c("SSE", "RSE", "RMSE", "R2")
```
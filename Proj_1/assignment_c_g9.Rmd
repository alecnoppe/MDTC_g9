---
title: "MDCE Assignment C, Group 9"
author: 
        - Quintijn de Leng - 6829376
        - Alec Noppe - 6947794
        - Guglielmo de Santis - 6664652
        - Anna Teixeira Rodeia - 6263747
date: "-4-2022"
output: pdf_document
fig_caption: true
hold_position: true
---
\newpage
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries, message=FALSE, warning=FALSE, echo = FALSE}
library(mice)
library(dplyr)
library(ggplot2)
library(naniar)
library(tidyr)
library(caret)
library(cowplot)
library(Hmisc)
library(Metrics)
library(miceadds)
library(knitr)
```

```{r, echo = FALSE}
dfcom <- readRDS("complete_data.rds")
dfinc <- readRDS("incomplete_data_g9.rds")
```

```{r include=FALSE}
prep_ini <- mice(dfinc, maxit = 0)
prep_ini_meth <- prep_ini$method
prep_ini_meth[c("smoke", "active", "height", "bmi")] <- ""
prep_ini_meth["weight"] <- "~ I(bmi * (height / 100)^2)"
imp_prep <- mice(dfinc,
                m = 1,
                maxit = 1,
                method = prep_ini_meth,
                print = FALSE)
dfinc <- complete(imp_prep)
```

# Introduction

In this report the attempt to predict active heart rate in an incomplete dataset, using multiple imputation. Our goal is to deduce whether the imputed dataset can produce similar regression results as the complete dataset, and that there is not a consistent flaw in the imputation. To do this, we compare the prediction with the complete dataset. 

In Assignment A we saw that after a recalculation of cases in variable *weight* that were not actually missing, 16.8% of the data was missing. The missing data pattern was connected and not monotone. 

Also referring back to Assignment A, we consider the Missing at Random-assumption to be plausible because of a non-significant Little's test and relatively close estimations of means and variances under listwise deletion.

# Modeling Decisions

## Imputation model

Predictive Mean Matching (pmm) is used for the numeric variables *active*, *height* and *weight*. For the binary variable *smoke*, logistic imputation regression is used.

*BMI* is a derived variable and thus, passive imputation was used to preserve the relationship between *height* and *weight* and to fill in the missing data of BMI. 

## Predictors

As there are relatively few variables in this dataset, all remaining variables are used to imputate any variable, with the exception of *BMI* on *weight* and *height* to prevent circularity. The loss of information in the imputations for the 49 cases where *BMI* is known, but not *height* and *weight* is not considered of any importance, since only *BMI* is used in the scientifically interesting model.

## Imputation order

The default order for imputations is used, as there is no clear reason to specify this otherwise. The missing data are not clearly close to monotone and thus do not benefit from this order and derived variable *BMI* already follows *height* and *weight* and thus the updated value is already taken into account in the following iteration.

## Number of iterations

5 iterations were chosen as the default to start with. We did not see a convergence of the model yet, so this was increased to 20; after which the model showed convergence (see Appendix, figure 1). This may be due to some obvious correlations between variables: *height* and *weight* are obviously related, and women are generally also shorter and have a lower weight.

## Number of imputed datasets

For the amount of imputations we followed the guideline to set this amount equal to the percentage of missing values (16.8%), rounded up to 17. We tried some higher and lower imputations as well, but this did not make a big impact on the plots. Because of this, we choose to keep the number of imputations equal to the percentage of missing values. 

## Final imputation model

Taking all of the former into account, the final imputation model looks like this:

```{r}
ini <- mice(dfinc, maxit = 0)
meth <- ini$method
meth["bmi"] <- "~ I(weight / (height / 100)^2)"
pred <- ini$predictorMatrix
pred[c("weight", "height"), "bmi"] <- 0

imp1 <- mice(dfinc,
             m = 17,
             maxit = 20,
             method = meth,
             predictorMatrix = pred,
             seed = 1337,
             print = FALSE)

```

Diagnostic plots of the imputations are offered in the Appendix. All point to a convergence of the model and realistic imputation outcomes.

## Means

In this section, means of the imputed, complete and incomplete dataset are compared. Only imputed numeric variables shown. As you cannot simply average the column means for each imputation *m*, the following calculation is used to come to the means:

```{r}
impmeans <- complete(imp1, "all") %>% 
            lapply(function(x) select(x, where(is.numeric)) %>% colMeans()) %>%
            do.call(rbind, .) %>%
            colMeans()
#From: Lang (2022, 3 December)
```



```{r, echo = FALSE, results = 'asis'}
impmeans <- as.data.frame(impmeans)
compmeans <- colMeans(dfcom[, c(1, 5:9)]) %>% 
        as.data.frame()
incmeans <- colMeans(dfinc[, c(1, 5:9)], na.rm = TRUE) %>% 
        as.data.frame()
meantable <- cbind(impmeans, compmeans, incmeans)
colnames(meantable) <- (c("imputed", "complete", "listwise"))
kable(round(meantable[c(2, 4:6), ], 2), caption = "Table of means in imputed, complete and incomplete dataset")
```

The mean of active heart rate in the imputed dataset is equal to the mean of the complete data. For *height*, the imputed dataset is actually further off than the listwise deletion method, as is slightly the case with *weight* (but overerstimating it instead of underestimating). However, this is still a minor difference. The imputed mean of *bmi* is closer to the complete data than the listwise-method is, although differences are very minor.

## Variances

In this section, the variance of the imputed, complete and incomplete dataset are compared. Only imputed numeric variables shown.

Variances for the imputed datasets are calculated as follows:

```{r}
impdat <- complete(imp1, action="long", include = FALSE)
pool_var <- with(impdat, by(impdat, .imp, function(x) c(var(x$active), 
                                                        var(x$height), 
                                                        var(x$weight), 
                                                        var(x$bmi))))
pool_var <- Reduce("+", pool_var) / length(pool_var)
#Adapted from: Heymans & Eekhout (2019)
```


```{r, echo = FALSE}
vartab <- as.data.frame(pool_var)


vartab$complete <- c(var(dfcom$active),
                     var(dfcom$height),
                     var(dfcom$weight),
                     var(dfcom$bmi))

vartab$listwise <- c(var(dfinc$active, na.rm = TRUE),
                     var(dfinc$height, na.rm = TRUE),
                     var(dfinc$weight, na.rm = TRUE),
                     var(dfinc$bmi, na.rm = TRUE))

colnames(vartab) <- c("imputed", "complete", "listwise")
rownames(vartab) <- c("active", "height", "weight", "bmi")
kable(round(vartab, 2), caption = "Table of variances for imputed, complete and incomplete data")
```

Unsurprisingly due to the added uncertainty of the missingness of the data, the variances are higher in the imputed datasets. However, the amount of added variance is relatively low. In the case of listwise deletion, an underestimation of the variance can even be seen in the event of *weight*.

## Correlations

In this section, matrices of the difference in correlations between the imputed and complete respectively the incomplete and complete datasets are shown.

```{r, echo = FALSE}
compcormat <- cor(dfcom[c(1, 5:9)])
inccormat <- cor(dfinc[c(1, 5:9)], use = "complete")
impcormat <- micombine.cor(mi.res = imp1, variables = c(1, 5:9))
impcormat <- attr(impcormat, "r_matrix")

diff1 <- impcormat - compcormat
diff2 <- inccormat - compcormat
diff3 <- diff1 - diff2
```


```{r, echo = FALSE}
kable(round((diff1), 2), caption = "Differences in correlations between the imputed and complete dataset")
```


```{r, echo = FALSE}
kable(round((diff2), 2), caption = "Differences in correlations between the incomplete and complete dataset")
```


Looking at the differences between correlations, we can see that the imputed dataset slightly underestimates the correlation between *weight* and *height*, perhaps because *bmi* was not used in their imputations. Overall however, the correlations are much more preserved in the imputed dataset over listwise deletion. 

## *Smoke* frequencies

In this section, the difference in frequency of the *smoke* variable are considered. Frequencies for the imputed dataset are calculated as follows:

```{r}
pool_count <- with(impdat, by(impdat, .imp, function(x) summary(x$smoke)))
pool_count <- Reduce("+", pool_count) / length(pool_count)
#Adapted from: Heymans & Eekhout (2019)
```


```{r, echo = FALSE}
counttab <- as.data.frame(pool_count)
counttab$complete <- summary(dfcom$smoke)
counttab$listwise <- summary(dfinc$smoke)[1:2]
counttab[nrow(counttab) + 1,] <- colSums(counttab)
colnames(counttab) <- c("imputed", "complete", "listwise")
rownames(counttab) <- c("no", "yes", "total")
#kable(round(counttab, 0), caption = "Frequency table for *smoke*")
```

```{r, echo = FALSE}
countperc <- counttab[c(1:2), ]
for (x in 1:3) {
        for (y in 1:2) {
                countperc[y, x] <- counttab[y, x] / counttab [3, x]
        } 
}
countperc <- countperc * 100
kable(round(countperc, 1), caption = "Relative frequencies for *smoke*")
```

In both listwise and MI the fraction of non-smokers is slightly overestimated. The differences are, however, very minor.

# Scientifically Interesting Model
After having imputed the missing data, we want to investigate the performance of a linear model trained on the imputed data, as well as the model trained on the complete data. The goal of this section is to deduce whether the imputed dataset can produce similar regression results as the complete dataset, and that there is not a consistent flaw in the imputation.
*Active heart rate* is dependent on many factors. For this model, *active heart rate* is dependent on *age*, *BMI*, *resting heart rate*, *gender*, *smoking*, *intensity* and an interaction between *BMI* and *age*. These are all the variables included in the dataset, with an additional interaction between BMI and age. This is suggested to increase the active heartrate exponentially, as opposed to a linear addition (Watkinson et al., 2010). The most influential variables are intensity, gender, and age. These variables have the highest slopes, as shown in the table below. The table also shows that we have lost quite some information to the missing data. Especially the rest stage, respondents with the gender female and respondents that smoke show a high fmi.

```{r, echo = FALSE}
fit <- with(imp1, lm(active ~ age + rest + bmi + sex + smoke + intensity + bmi * age))
est <- pool(fit)
a <- as.data.frame(summary(est))
a[,2:6] <- round(a[,2:6], 2)
colnames(a) <- c("term", "estimate", "SE", "statistic", "df", "p")
rownames(a) <- c("(Intercept)", "age", "rest", "bmi", "sex (female)", "smoke (yes)", "intenstiy (moderate)", "intenstiy (low)", "age:bmi")
kable(a[,2:6])
est
```

 

To assess a model's performance, it is important to evaluate the accuracy of the model on seen values (training validation) as well as the accuracy of the model on new data (testing validation). The reason for this is that a model may perform well on data that has been seen before, but not on new data. The metrics used to determine the performance of the model are the sum of squares error *SSE*, relative standard error *RSE*, root mean square error *RMSE* and *R^2*. For the former three metrics, a lower score is ideal. Whereas for the R^2, a score closest to 1 is ideal.

 

## Training Validation

 

As seen below, the model trained on the imputed dataset produces less accurate results than the model trained on the complete dataset. However, both models have a R^2 score of only 0.51 and 0.53 respectively. This means that only roughly 50% of the variation of *activity* is explained through the independent variables. This implies that there are latent variables that explain some of the activity variance. Nonetheless, the imputed dataset model performs similarly to the complete dataset model.

```{r, echo = FALSE}
smp_size <- floor(0.75 * nrow(dfcom))

 

## set the seed to make your partition reproducible
set.seed(123)
train_ind <- sample(seq_len(nrow(dfcom)), size = smp_size)

 

train.com <- dfcom[train_ind, ]
test.com <- dfcom[-train_ind, ]

 

train.imp <- complete(imp1)[train_ind, ]
test.imp <- complete(imp1)[-train_ind, ]

 

model.com <- lm(active ~ age + rest + bmi + sex + smoke + intensity + bmi * age, data=train.com)
pred.com <- predict(model.com, newdata=train.com)

 

model.imp <- lm(active ~ age + rest + bmi + sex + smoke + intensity + bmi * age, data=train.imp)
pred.imp <- predict(model.imp, newdata=train.imp)

 

tActivity <- train.com$active
mActivity <- pred.imp

 

test.com.active<- tActivity
pred.com.error <- tActivity - mActivity
pred.com.R2=1-sum(pred.com.error^2)/sum((test.com.active- mean(test.com.active))^2)
eval.imp <- c(sse(tActivity, mActivity), rse(tActivity, mActivity), rmse(tActivity, mActivity), pred.com.R2)

 

tActivity <- train.com$active
mActivity <- pred.com

 

test.com.active<- tActivity
pred.com.error <- tActivity - mActivity
pred.com.R2=1-sum(pred.com.error^2)/sum((test.com.active- mean(test.com.active))^2)
eval.com <- c(sse(tActivity, mActivity), rse(tActivity, mActivity), rmse(tActivity, mActivity), pred.com.R2)

 

eval.full <- data.frame(t(eval.imp), row.names = c("Imputed Data"))
colnames(eval.full) <- c("SSE", "RSE", "RMSE", "R2")
eval.full["Complete Data",] = eval.com
eval.full
```

 


## Testing Validation

The imputed dataset model has a slightly higher training accuracy than the complete dataset. It has a R^2 of 0.468, whereas the complete dataset model has a R^2 of 0.465. The difference is too small to make any definitive claims on the relative testing accuracy between the two models. But this does suggest that the imputed dataset model yields similar results to the complete dataset model. Thus, the multiple imputation as performed in this research successfully fills in the missing values to an extent that yields proper results for our scientifically interesting model.
A final remark regarding the training and test scores is that both models show some signs of being overfitted to the training data. The R^2 of the models validated on training accuracy are up to 5% greater than the models validated on testing accuracy. This is a sign of overfitting.
```{r, echo = FALSE}

 

model.com <- lm(active ~ age + rest + bmi + sex + smoke + intensity + bmi * age, data=train.com)
pred.com <- predict(model.com, newdata=test.com)

 

model.imp <- lm(active ~ age + rest + bmi + sex + smoke + intensity + bmi * age, data=train.imp)
pred.imp <- predict(model.imp, newdata=test.imp)

 

tActivity <- test.com$active
mActivity <- pred.imp

 

test.com.active<- tActivity
pred.com.error <- tActivity - mActivity
pred.com.R2=1-sum(pred.com.error^2)/sum((test.com.active- mean(test.com.active))^2)
eval.imp <- c(sse(tActivity, mActivity), rse(tActivity, mActivity), rmse(tActivity, mActivity), pred.com.R2)

 

tActivity <- test.com$active
mActivity <- pred.com

 

test.com.active<- tActivity
pred.com.error <- tActivity - mActivity
pred.com.R2=1-sum(pred.com.error^2)/sum((test.com.active- mean(test.com.active))^2)
eval.com <- c(sse(tActivity, mActivity), rse(tActivity, mActivity), rmse(tActivity, mActivity), pred.com.R2)

 

eval.full <- data.frame(t(eval.imp), row.names = c("Imputed Data"))
colnames(eval.full) <- c("SSE", "RSE", "RMSE", "R2")
eval.full["Complete Data",] = eval.com
eval.full
```

 

## Analysis of Variance

Finally, an Analysis of variance *ANOVA* analysis is performed on both datasets to conclude whether there is a significant difference in means between variables. ANOVA is a statistical technique that is used to check if the means of two or more groups are significantly different from each other. In our dataset, the most significant variables in terms of mean scores are age and resting heart rate. These variables have the highest F-values and the lowest p-values. Other variables show noteworthy variance, but none have a p-score lower than the significance level (0.05).
When comparing the two ANOVA regressions, the explained variance of the linear model trained on both datasets are remarkably similar. The residual variance is within 2% of each other, and thus we conclude that the multiple imputation produces a reasonable dataset.

ANOVA table for imputed data:
```{r, echo = FALSE}
a <- miceadds::mi.anova(imp1, "active ~ age + rest + bmi + sex + smoke + intensity + bmi*age", type=2)
b <- summary(aov(active ~ rest + bmi + sex + intensity + bmi*age, dfcom))
```

ANOVA table for complete data:
```{r, echo = FALSE}
b
```


\newpage
## Sources

- Chunk 9 (imputation model means): Lang, K. (2022, 3 december). *Missing Data Theory & Causal Effects, practical 6, sec. 2.2* [R code]. 
- Chunks 11, 16 (imputation model variances; *smoke* frequencies): Heymans, M.W. & Eekhout, I. (2019). *Applied Missing Data Analysis with SPSS and Rstudio*, section 5.2.2 [R code].
- Watkinson, C., van Sluijs, E. M., Sutton, S., Hardeman, W., Corder, K., & Griffin, S. J. (2010). Overestimation of physical activity level is associated with lower BMI: a cross-sectional analysis. *International journal of behavioral nutrition and physical activity, 7*(1), 1-9.

\newpage

## Appendix: diagnostic figures

```{r convergence, echo = FALSE, fig.height = 6.7, fig.cap = "Convergence of imputed data"}
plot(imp1,
     layout = c(2, 5))
```

```{r densityplot, echo = FALSE, out.width = "90%", fig.cap = "Density plot of imputed data"}
densityplot(imp1)
```

```{r stripplot, echo = FALSE, out.width = "90%", fig.cap = "Stripplot of imputed data"}
stripplot(imp1)
```
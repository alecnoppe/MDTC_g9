---
title: "MDCE Assignment B, Group 9"
author: 
        - Quintijn de Leng - 6829376
        - Alec Noppe - 6947794
        - Guglielmo de Santis - 6664652
        - Anna Teixeira Rodeia - 6263747
date: "1-4-2022"
output: pdf_document
---
\newpage
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries, message=FALSE, warning=FALSE, echo = FALSE}
library(mice)
library(dplyr)
library(ggplot2)
library(naniar)
library(tidyr)
library(caret)
library(cowplot)
library(Hmisc)
library(Metrics)
library(miceadds)
```

```{r, echo = FALSE}
dfcom <- readRDS("complete_data.rds")
dfinc <- readRDS("incomplete_data_g9.rds")
```

# Preparing the dataset for imputation

As in the last assignment, we re-calculate some missing cases of *weight* to reflect the actual non-missingness of these cases. This time, we avoid recalculating all cases of *weight* using the following syntax:

```{r}
prep_ini <- mice(dfinc, maxit = 0)
prep_ini_meth <- prep_ini$method
prep_ini_meth[c("smoke", "active", "height", "bmi")] <- ""
prep_ini_meth["weight"] <- "~ I(bmi * (height / 100)^2)"
imp_prep <- mice(dfinc,
                m = 1,
                maxit = 1,
                method = prep_ini_meth,
                print = FALSE)
dfinc <- complete(imp_prep)
```

# Imputation

To impute the missing data, we used the mice algorithm. For the amount of imputations we followed the guideline to set this amount equal to the percentage of missing values (16.8%), rounded up to 17. 

- For the binary variable *smoke*, logistic regression imputation was used. 
- For the numeric variables *active*, *height* and *weight*, predictive mean matching (pmm) was used. 
- *BMI* was imputed by passive imputation to preserve the relationsip between *height* and *weight*. To prevent any problems with circularity in the imputations, *bmi* is not used as a predictor for the imputations of *height* or *weight*. 


```{r}
ini <- mice(dfinc, maxit = 0)
meth <- ini$method
meth["bmi"] <- "~ I(weight / (height / 100)^2)"
pred <- ini$predictorMatrix
pred[c("weight", "height"), "bmi"] <- 0

imp1 <- mice(dfinc,
             m = 17,
             maxit = 20,
             method = meth,
             predictorMatrix = pred,
             seed = 1337,
             print = FALSE)

```


Using five iterations, we did not yet see a convergence in some variables. This may be because of the relatively high correlation between some variables, most notably (and obviously) *weight* and *height*. After 10 iterations, all variables showed convergence (see below).

## Diagnostics

```{r, echo = FALSE}
plot(imp1)
```

Density plot of imputed data:

```{r, echo = FALSE, out.width = "80%"}
densityplot(imp1)
```

Stripplot of imputed data:

```{r echo = FALSE, out.width = "80%"}
stripplot(imp1)
```

These plots look good and show convergence. The stripplots show realistic values for the imputed data. *bmi* is the only variable that does not exactly match observed values due to the nature of passive imputation. 

# Comparison of imputed and complete data

## Means

In this section, means of the imputed, complete and incomplete dataset are compared. Only imputed numeric variables shown. As you cannot simply average the column means for each imputation *m*, the following calculation is used to come to the means:

```{r}
impmeans <- complete(imp1, "all") %>% 
            lapply(function(x) select(x, where(is.numeric)) %>% colMeans()) %>%
            do.call(rbind, .) %>%
            colMeans()
#From: Lang (2022, 3 December)
```

Table of means in imputed, complete and incomplete dataset:

```{r, echo = FALSE}
impmeans <- as.data.frame(impmeans)
compmeans <- colMeans(dfcom[, c(1, 5:9)]) %>% 
        as.data.frame()
incmeans <- colMeans(dfinc[, c(1, 5:9)], na.rm = TRUE) %>% 
        as.data.frame()
meantable <- cbind(impmeans, compmeans, incmeans)
colnames(meantable) <- (c("imputed", "complete", "listwise"))
round(meantable[c(2, 4:6), ], 2)
```

The mean of active heart rate in the imputed dataset is equal to the mean of the complete data. For *height*, the imputed dataset is actually further off than the listwise deletion method, as is slightly the case with *weight* (but overerstimating it instead of underestimating). However, this is still a minor difference. The imputed mean of *bmi* is closer to the complete data than the listwise-method is, although differences are very minor.

## Variances

In this section, the variance of the imputed, complete and incomplete dataset are compared. Only imputed numeric variables shown.

Variances for the imputed datasets are calculated as follows:

```{r}
impdat <- complete(imp1, action="long", include = FALSE)
pool_var <- with(impdat, by(impdat, .imp, function(x) c(var(x$active), 
                                                        var(x$height), 
                                                        var(x$weight), 
                                                        var(x$bmi))))
pool_var <- Reduce("+", pool_var) / length(pool_var)
#Adapted from: Heymans & Eekhout (2019)
```

Table of variances for imputed, complete and incomplete data:

```{r, echo = FALSE}
vartab <- as.data.frame(pool_var)


vartab$complete <- c(var(dfcom$active),
                     var(dfcom$height),
                     var(dfcom$weight),
                     var(dfcom$bmi))

vartab$listwise <- c(var(dfinc$active, na.rm = TRUE),
                     var(dfinc$height, na.rm = TRUE),
                     var(dfinc$weight, na.rm = TRUE),
                     var(dfinc$bmi, na.rm = TRUE))

colnames(vartab) <- c("imputed", "complete", "listwise")
rownames(vartab) <- c("active", "height", "weight", "bmi")
round(vartab, 2)
```

Unsurprisingly due to the added uncertainty of the missingness of the data, the variances are higher in the imputed datasets. However, the amount of added variance is relatively low. In the case of listwise deletion, an underestimation of the variance can even be seen in the event of *weight*.

## Correlations

In this section, matrices of the difference in correlations between the imputed and complete respectively the incomplete and complete datasets are shown.

```{r, echo = FALSE}
compcormat <- cor(dfcom[c(1, 5:9)])
inccormat <- cor(dfinc[c(1, 5:9)], use = "complete")
impcormat <- micombine.cor(mi.res = imp1, variables = c(1, 5:9))
impcormat <- attr(impcormat, "r_matrix")
```

Differences in correlations between the imputed and complete dataset:

```{r, echo = FALSE}
round((impcormat - compcormat), 2)
```

Differences in correlations between the incomplete and complete dataset:

```{r, echo = FALSE}
round((inccormat - compcormat), 2)
```

Looking at the differences between correlations, we can see that the imputed dataset slightly underestimates the correlation between *weight* and *height*, perhaps because *bmi* was not used in their imputations. Overall however, the correlations are much more preserved in the imputed dataset over listwise deletion.

## *Smoke* frequencies

In this section, the difference in frequency of the *smoke* variable are considered. Frequencies for the imputed dataset are calculated as follows:

```{r}
pool_count <- with(impdat, by(impdat, .imp, function(x) summary(x$smoke)))
pool_count <- Reduce("+", pool_count) / length(pool_count)
#Adapted from: Heymans & Eekhout (2019)
```

Frequency table for *smoke*

```{r, echo = FALSE}
counttab <- as.data.frame(pool_count)
counttab$complete <- summary(dfcom$smoke)
counttab$listwise <- summary(dfinc$smoke)[1:2]
counttab[nrow(counttab) + 1,] <- colSums(counttab)
colnames(counttab) <- c("imputed", "complete", "listwise")
rownames(counttab) <- c("no", "yes", "total")
round(counttab, 0)
```

The binary variable *smoke* is represented in a much better way than in the case of listwise deletion.

# Scientifically Interesting Model
After having imputed the missing data, we want to investigate the performance of a linear model trained on the imputed data, as well as the model trained on the complete data. The goal of this section is to deduce whether the imputed dataset can produce similar regression results as the complete dataset, and that there is not a consistent flaw in the imputation.
*Active heart rate* is dependent on many factors. For this model, *active heart rate* is dependent on *age*, *BMI*, *resting heart rate*, *gender*, *smoking*, *intensity* and an interaction between *BMI* and *age*. These are all the variables included in the dataset, with an additional interaction between BMI and age. This is suggested to increase the active heartrate exponentially, as opposed to a linear addition (Watkinson et al., 2010). The most influential variables are intensity, gender, and age. These variables have the highest slopes, as shown in the table below. 

 

```{r, echo = FALSE}
fit <- with(imp1, lm(active ~ age + rest + bmi + sex + smoke + intensity + bmi * age))
est <- pool(fit)
a <- as.data.frame(summary(est))
a[,2:6] <- round(a[,2:6], 2)
colnames(a) <- c("term", "estimate", "SE", "statistic", "df", "p")
rownames(a) <- c("(Intercept)", "age", "rest", "bmi", "sex (female)", "smoke (yes)", "intenstiy (moderate)", "intenstiy (low)", "age:bmi")
a[,2:6]
```

 

To assess a model's performance, it is important to evaluate the accuracy of the model on seen values (training validation) as well as the accuracy of the model on new data (testing validation). The reason for this is that a model may perform well on data that has been seen before, but not on new data. The metrics used to determine the performance of the model are the sum of squares error *SSE*, relative standard error *RSE*, root mean square error *RMSE* and *R^2*. For the former three metrics, a lower score is ideal. Whereas for the R^2, a score closest to 1 is ideal.

 

## Training Validation

 

As seen below, the model trained on the imputed dataset produces less accurate results than the model trained on the complete dataset. However, both models have a R^2 score of only 0.51 and 0.53 respectively. This means that only roughly 50% of the variation of *activity* is explained through the independent variables. This implies that there are latent variables that explain some of the activity variance. Nonetheless, the imputed dataset model performs similarly to the complete dataset model.

```{r, echo = FALSE}
smp_size <- floor(0.75 * nrow(dfcom))

 

## set the seed to make your partition reproducible
set.seed(123)
train_ind <- sample(seq_len(nrow(dfcom)), size = smp_size)

 

train.com <- dfcom[train_ind, ]
test.com <- dfcom[-train_ind, ]

 

train.imp <- complete(imp1)[train_ind, ]
test.imp <- complete(imp1)[-train_ind, ]

 

model.com <- lm(active ~ age + rest + bmi + sex + smoke + intensity + bmi * age, data=train.com)
pred.com <- predict(model.com, newdata=train.com)

 

model.imp <- lm(active ~ age + rest + bmi + sex + smoke + intensity + bmi * age, data=train.imp)
pred.imp <- predict(model.imp, newdata=train.imp)

 

tActivity <- train.com$active
mActivity <- pred.imp

 

test.com.active<- tActivity
pred.com.error <- tActivity - mActivity
pred.com.R2=1-sum(pred.com.error^2)/sum((test.com.active- mean(test.com.active))^2)
eval.imp <- c(sse(tActivity, mActivity), rse(tActivity, mActivity), rmse(tActivity, mActivity), pred.com.R2)

 

tActivity <- train.com$active
mActivity <- pred.com

 

test.com.active<- tActivity
pred.com.error <- tActivity - mActivity
pred.com.R2=1-sum(pred.com.error^2)/sum((test.com.active- mean(test.com.active))^2)
eval.com <- c(sse(tActivity, mActivity), rse(tActivity, mActivity), rmse(tActivity, mActivity), pred.com.R2)

 

eval.full <- data.frame(t(eval.imp), row.names = c("Imputed Data"))
colnames(eval.full) <- c("SSE", "RSE", "RMSE", "R2")
eval.full["Complete Data",] = eval.com
eval.full
```

 


## Testing Validation

The imputed dataset model has a slightly higher training accuracy than the complete dataset. It has a R^2 of 0.468, whereas the complete dataset model has a R^2 of 0.465. The difference is too small to make any definitive claims on the relative testing accuracy between the two models. But this does suggest that the imputed dataset model yields similar results to the complete dataset model. Thus, the multiple imputation as performed in this research successfully fills in the missing values to an extent that yields proper results for our scientifically interesting model.
A final remark regarding the training and test scores is that both models show some signs of being overfitted to the training data. The R^2 of the models validated on training accuracy are up to 5% greater than the models validated on testing accuracy. This is a sign of overfitting.
```{r, echo = FALSE}

 

model.com <- lm(active ~ age + rest + bmi + sex + smoke + intensity + bmi * age, data=train.com)
pred.com <- predict(model.com, newdata=test.com)

 

model.imp <- lm(active ~ age + rest + bmi + sex + smoke + intensity + bmi * age, data=train.imp)
pred.imp <- predict(model.imp, newdata=test.imp)

 

tActivity <- test.com$active
mActivity <- pred.imp

 

test.com.active<- tActivity
pred.com.error <- tActivity - mActivity
pred.com.R2=1-sum(pred.com.error^2)/sum((test.com.active- mean(test.com.active))^2)
eval.imp <- c(sse(tActivity, mActivity), rse(tActivity, mActivity), rmse(tActivity, mActivity), pred.com.R2)

 

tActivity <- test.com$active
mActivity <- pred.com

 

test.com.active<- tActivity
pred.com.error <- tActivity - mActivity
pred.com.R2=1-sum(pred.com.error^2)/sum((test.com.active- mean(test.com.active))^2)
eval.com <- c(sse(tActivity, mActivity), rse(tActivity, mActivity), rmse(tActivity, mActivity), pred.com.R2)

 

eval.full <- data.frame(t(eval.imp), row.names = c("Imputed Data"))
colnames(eval.full) <- c("SSE", "RSE", "RMSE", "R2")
eval.full["Complete Data",] = eval.com
eval.full
```

 

## Analysis of Variance

Finally, an Analysis of variance *ANOVA* analysis is performed on both datasets to conclude whether there is a significant difference in means between variables. ANOVA is a statistical technique that is used to check if the means of two or more groups are significantly different from each other. In our dataset, the most significant variables in terms of mean scores are age and resting heart rate. These variables have the highest F-values and the lowest p-values. Other variables show noteworthy variance, but none have a p-score lower than the significance level (0.05).
When comparing the two ANOVA regressions, the explained variance of the linear model trained on both datasets are remarkably similar. The residual variance is within 2% of each other, and thus we conclude that the multiple imputation produces a reasonable dataset.

ANOVA table for imputed data:
```{r, echo = FALSE}
a <- miceadds::mi.anova(imp1, "active ~ age + rest + bmi + sex + smoke + intensity + bmi*age", type=2)
b <- summary(aov(active ~ rest + bmi + sex + intensity + bmi*age, dfcom))
```

ANOVA table for complete data:
```{r, echo = FALSE}
b
```


\newpage
## Sources

- Chunk 9 (imputation model means): Lang, K. (2022, 3 december). *Missing Data Theory & Causal Effects, practical 6, sec. 2.2* [R code]. 
- Chunks 11, 16 (imputation model variances; *smoke* frequencies): Heymans, M.W. & Eekhout, I. (2019). *Applied Missing Data Analysis with SPSS and Rstudio*, section 5.2.2 [R code].
- Watkinson, C., van Sluijs, E. M., Sutton, S., Hardeman, W., Corder, K., & Griffin, S. J. (2010). Overestimation of physical activity level is associated with lower BMI: a cross-sectional analysis. *International journal of behavioral nutrition and physical activity, 7*(1), 1-9.